{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\torchaudio\\extension\\extension.py:13: UserWarning: torchaudio C++ extension is not available.\n",
      "  warnings.warn('torchaudio C++ extension is not available.')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import *\n",
    "from tensorflow.keras.callbacks import  ModelCheckpoint\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report, accuracy_score\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor Flow Version: 2.5.0\n",
      "Keras Version: 2.5.0\n",
      "\n",
      "Python 3.8.3 (default, Jul  2 2020, 17:30:36) [MSC v.1916 64 bit (AMD64)]\n",
      "Pandas 1.0.5\n",
      "Scikit-Learn 0.23.1\n",
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "import tensorflow.keras\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "print(f\"Tensor Flow Version: {tf.__version__}\")\n",
    "print(f\"Keras Version: {tensorflow.keras.__version__}\")\n",
    "print()\n",
    "print(f\"Python {sys.version}\")\n",
    "print(f\"Pandas {pd.__version__}\")\n",
    "print(f\"Scikit-Learn {sk.__version__}\")\n",
    "gpu = len(tf.config.list_physical_devices('GPU'))>0\n",
    "print(\"GPU is\", \"available\" if gpu else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random seed 고정\n",
    "tf.random.set_seed(1234)\n",
    "np.random.seed(1234)\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "NUM_EPOCHS = 20\n",
    "MAX_LEN = 512 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "개체명 인식 학습 데이터 개수: 21525\n",
      "개체명 인식 테스트 데이터 개수: 5382\n"
     ]
    }
   ],
   "source": [
    "DATA_TRAIN_PATH = os.path.join(\"train_path\")\n",
    "DATA_TEST_PATH = os.path.join(\"test_path\")\n",
    "\n",
    "\n",
    "def read_file(input_path):\n",
    "    \"\"\"Read tsv file, and return words and label as list\"\"\"\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        sentences = []\n",
    "        labels = []\n",
    "        for line in f:\n",
    "            split_line = line.strip().split(\"\\t\")\n",
    "            sentences.append(split_line[0])\n",
    "            labels.append(split_line[1])\n",
    "        return sentences[1:], labels[1:]\n",
    "\n",
    "train_sentences, train_labels = read_file(DATA_TRAIN_PATH)\n",
    "for i in range(len(train_labels)):\n",
    "    train_labels[i] = train_labels[i][1:-1].replace('\\'','').replace(' ','').split(\",\")\n",
    "train_ner_dict = {\"sentence\": train_sentences, \"label\": train_labels}\n",
    "train_ner_df = pd.DataFrame(train_ner_dict)\n",
    "\n",
    "test_sentences, test_labels = read_file(DATA_TEST_PATH)\n",
    "for i in range(len(test_labels)):\n",
    "    test_labels[i] = test_labels[i][1:-1].replace('\\'','').replace(' ','').split(\",\")\n",
    "test_ner_dict = {\"sentence\": test_sentences, \"label\": test_labels}\n",
    "test_ner_df = pd.DataFrame(test_ner_dict)\n",
    "\n",
    "print(\"개체명 인식 학습 데이터 개수: {}\".format(len(train_ner_df)))\n",
    "print(\"개체명 인식 테스트 데이터 개수: {}\".format(len(test_ner_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "개체명 인식 학습 데이터 개수: 21500\n",
      "개체명 인식 테스트 데이터 개수: 5378\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(train_ner_df)):\n",
    "    if len(train_ner_df['sentence'][i]) > 512:\n",
    "        train_ner_df = train_ner_df.drop(index=i, axis=0)\n",
    "\n",
    "for i in range(len(test_ner_df)):\n",
    "    if len(test_ner_df['sentence'][i]) > 512:\n",
    "        test_ner_df = test_ner_df.drop(index=i, axis=0)\n",
    "\n",
    "train_ner_df = train_ner_df.reset_index(drop=True)\n",
    "test_ner_df = test_ner_df.reset_index(drop=True)\n",
    "    \n",
    "print(\"개체명 인식 학습 데이터 개수: {}\".format(len(train_ner_df)))\n",
    "print(\"개체명 인식 테스트 데이터 개수: {}\".format(len(test_ner_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_labels = ['UNK', 'O', 'B-PDT', 'I-PDT', 'B-MOV', 'I-MOV', 'B-TRV', 'I-TRV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 버트 토크나이저 설정\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\", cache_dir='bert_ckpt')\n",
    "\n",
    "pad_token_id = tokenizer.pad_token_id # 0\n",
    "pad_token_label_id = 0\n",
    "cls_token_label_id = 0\n",
    "sep_token_label_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_tokenizer(sent, MAX_LEN):\n",
    "    \n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        text = sent,\n",
    "        truncation=True,\n",
    "        add_special_tokens = True, #'[CLS]'와 '[SEP]' 추가\n",
    "        max_length = MAX_LEN,           # 문장 패딩 및 자르기 진행\n",
    "        pad_to_max_length = True,\n",
    "        return_attention_mask = True   # 어탠션 마스크 생성\n",
    "    )\n",
    "    \n",
    "    input_id = encoded_dict['input_ids']\n",
    "    attention_mask = encoded_dict['attention_mask'] \n",
    "    token_type_id = encoded_dict['token_type_ids']\n",
    "    \n",
    "    return input_id, attention_mask, token_type_id\n",
    "\n",
    "def convert_label(words, labels_idx, ner_begin_label, max_seq_len):\n",
    "            \n",
    "    tokens = []\n",
    "    label_ids = []\n",
    "\n",
    "    for word, slot_label in zip(words, labels_idx):\n",
    "        word_tokens = tokenizer.tokenize(word)\n",
    "        tokens.extend(word_tokens)\n",
    "        \n",
    "        # 슬롯 레이블 값이 Begin이면 I로 추가\n",
    "        if int(slot_label) in ner_begin_label:\n",
    "            label_ids.extend([int(slot_label)] + [int(slot_label) + 1] * (len(word_tokens) - 1))\n",
    "        else:\n",
    "            label_ids.extend([int(slot_label)] * len(word_tokens))\n",
    "  \n",
    "    # [CLS] and [SEP] 설정\n",
    "    special_tokens_count = 2\n",
    "    if len(label_ids) > max_seq_len - special_tokens_count:\n",
    "        label_ids = label_ids[: (max_seq_len - special_tokens_count)]\n",
    "\n",
    "    # [SEP] 토큰 추가\n",
    "    label_ids += [sep_token_label_id]\n",
    "\n",
    "    # [CLS] 토큰 추가\n",
    "    label_ids = [cls_token_label_id] + label_ids\n",
    "    \n",
    "    padding_length = max_seq_len - len(label_ids)\n",
    "    label_ids = label_ids + ([pad_token_label_id] * padding_length)\n",
    "    \n",
    "    return label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6]\n",
      "['B-PDT', 'B-MOV', 'B-TRV']\n"
     ]
    }
   ],
   "source": [
    "# 테스트용\n",
    "ner_begin_label = [ner_labels.index(begin_label) for begin_label in ner_labels if \"B\" in begin_label]\n",
    "ner_begin_label_string = [ner_labels[label_index] for label_index in ner_begin_label]\n",
    "\n",
    "print(ner_begin_label)\n",
    "print(ner_begin_label_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_begin_label = [ner_labels.index(begin_label) for begin_label in ner_labels if \"B\" in begin_label]\n",
    "\n",
    "def create_inputs_targets(df):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    token_type_ids = []\n",
    "    label_list = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        sentence, labels = df['sentence'][i], df['label'][i]\n",
    "        words = sentence.split()\n",
    "        labels_idx = []\n",
    "\n",
    "        for label in labels:\n",
    "            labels_idx.append(ner_labels.index(label) if label in ner_labels else ner_labels.index(\"UNK\"))\n",
    "\n",
    "\n",
    "        #assert len(words) == len(labels_idx)\n",
    "\n",
    "        input_id, attention_mask, token_type_id = bert_tokenizer(sentence, MAX_LEN)\n",
    "\n",
    "        convert_label_id = convert_label(words, labels_idx, ner_begin_label, MAX_LEN)\n",
    "\n",
    "        input_ids.append(input_id)\n",
    "        attention_masks.append(attention_mask)\n",
    "        token_type_ids.append(token_type_id)\n",
    "        label_list.append(convert_label_id)\n",
    "\n",
    "    input_ids = np.array(input_ids, dtype=int)\n",
    "    attention_masks = np.array(attention_masks, dtype=int)\n",
    "    token_type_ids = np.array(token_type_ids, dtype=int)\n",
    "    label_list = np.asarray(label_list, dtype=int) #레이블 토크나이징 리스트\n",
    "    inputs = (input_ids, attention_masks, token_type_ids)\n",
    "    \n",
    "    return inputs, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_inputs, train_labels = create_inputs_targets(train_ner_df)\n",
    "test_inputs, test_labels = create_inputs_targets(test_ner_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "\n",
    "class CRF(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, units=None, chain_initializer=\"orthogonal\", regularizer=None, **kwargs):\n",
    "        super(CRF, self).__init__(**kwargs)\n",
    "        self.chain_initializer = tf.keras.initializers.get(chain_initializer)\n",
    "        self.regularizer = regularizer\n",
    "        self.transitions = None\n",
    "        self.supports_masking = True\n",
    "        self.mask = None\n",
    "        self.accuracy_fn = tf.keras.metrics.Accuracy()\n",
    "        self.units = units\n",
    "        if units is not None:\n",
    "            self.dense = tf.keras.layers.Dense(units)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(CRF, self).get_config()\n",
    "        config.update({\n",
    "            \"chain_initializer\": \"orthogonal\"\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        if self.units:\n",
    "            units = self.units\n",
    "        else:\n",
    "            units = input_shape[-1]\n",
    "        self.transitions = self.add_weight(\n",
    "            name=\"transitions\",\n",
    "            shape=[units, units],\n",
    "            initializer=self.chain_initializer,\n",
    "            regularizer=self.regularizer\n",
    "        )\n",
    "\n",
    "    def call(self, inputs, mask=None, training=False):\n",
    "        if mask is None:\n",
    "            raw_input_shape = tf.slice(tf.shape(inputs), [0], [2])\n",
    "            mask = tf.ones(raw_input_shape)\n",
    "        sequence_lengths = K.sum(K.cast(mask, 'int32'), axis=-1)\n",
    "        if self.units:\n",
    "            inputs = self.dense(inputs)\n",
    "        viterbi_sequence, _ = tfa.text.crf_decode(\n",
    "            inputs, self.transitions, sequence_lengths\n",
    "        )\n",
    "        return viterbi_sequence, inputs, sequence_lengths, self.transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow_addons.text.crf import crf_log_likelihood\n",
    "\n",
    "\n",
    "def unpack_data(data):\n",
    "    if len(data) == 2:\n",
    "        return data[0], data[1], None\n",
    "    elif len(data) == 3:\n",
    "        return data\n",
    "    else:\n",
    "        raise TypeError(\"Expected data to be a tuple of size 2 or 3.\")\n",
    "\n",
    "\n",
    "class ModelWithCRFLoss(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Wrapper around the base model for custom training logic.\n",
    "    Args:\n",
    "        base_model: The model including the CRF layer\n",
    "        sparse_target: if the y label is sparse or one-hot, default True\n",
    "        metric: the metric for training, default 'accuracy'. Warning: Currently tensorflow metrics like AUC need the output and y_true to be one-hot to cauculate, they are not supported.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_model, sparse_target=True, metric: Union[str, object] = 'accuracy'):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.sparse_target = sparse_target\n",
    "        \n",
    "        self.metric = metric\n",
    "        if isinstance(metric, str):\n",
    "            if metric == 'accuracy':\n",
    "                self.metrics_fn = tf.keras.metrics.Accuracy(name='accuracy')\n",
    "            else:\n",
    "                raise ValueError('unknown metric name')\n",
    "        else:\n",
    "            self.metrics_fn = self.metric\n",
    "        self.loss_tracker = tf.keras.metrics.Mean(name='loss')\n",
    "\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        if training:\n",
    "            return self.base_model(inputs)\n",
    "        else:\n",
    "            return self.base_model(inputs)[0]\n",
    "\n",
    "    def compute_loss(self, x, y, training=False):\n",
    "        viterbi_sequence, potentials, sequence_length, chain_kernel = self(x, training=training)\n",
    "        # we now add the CRF loss:\n",
    "        crf_loss = -crf_log_likelihood(potentials, y, sequence_length, chain_kernel)[0]\n",
    "        return viterbi_sequence, sequence_length, tf.reduce_mean(crf_loss)\n",
    "\n",
    "\n",
    "    def train_step(self, data):\n",
    "        x, y, sample_weight = unpack_data(data)\n",
    "        # y : '(batch_size, seq_length)'\n",
    "        if self.sparse_target:\n",
    "            assert len(y.shape) == 2\n",
    "        else:\n",
    "            y = tf.argmax(y, axis=-1)\n",
    "        with tf.GradientTape() as tape:\n",
    "            viterbi_sequence, sequence_length, crf_loss = self.compute_loss(x, y, training=True)\n",
    "            loss = crf_loss + tf.cast(tf.reduce_sum(self.losses), crf_loss.dtype)\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.metrics_fn.update_state(y, viterbi_sequence, tf.sequence_mask(sequence_length, y.shape[1]))\n",
    "        return {\"loss\": self.loss_tracker.result(), self.metrics_fn.name: self.metrics_fn.result()}\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_tracker, self.metrics_fn]\n",
    "\n",
    "    def test_step(self, data):\n",
    "        x, y, sample_weight = unpack_data(data)\n",
    "        # y : '(batch_size, seq_length)'\n",
    "        if self.sparse_target:\n",
    "            assert len(y.shape) == 2\n",
    "        else:\n",
    "            y = tf.argmax(y, axis=-1)\n",
    "        viterbi_sequence, sequence_length, crf_loss = self.compute_loss(x, y, training=True)\n",
    "        loss = crf_loss + tf.cast(tf.reduce_sum(self.losses), crf_loss.dtype)\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.metrics_fn.update_state(y, viterbi_sequence, tf.sequence_mask(sequence_length, y.shape[1]))\n",
    "        return {\"loss_val\": self.loss_tracker.result(), f'val_{self.metrics_fn.name}': self.metrics_fn.result()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFBertNERClassifier(tf.keras.Model):\n",
    "    def __init__(self, model_name, dir_path, num_class):\n",
    "        super(TFBertNERClassifier, self).__init__()\n",
    "\n",
    "        self.bert = TFBertModel.from_pretrained(model_name, cache_dir=dir_path)\n",
    "        self.dropout = tf.keras.layers.Dropout(self.bert.config.hidden_dropout_prob)\n",
    "        self.classifier = tf.keras.layers.Dense(num_class, \n",
    "                                                kernel_initializer=tf.keras.initializers.TruncatedNormal(self.bert.config.initializer_range),\n",
    "                                                name=\"ner_classifier\")\n",
    "        self.crf = CRF(units=8, name='crf')\n",
    "\n",
    "    def call(self, inputs, attention_mask=None, token_type_ids=None, training=False):\n",
    "\n",
    "        #outputs 값: # sequence_output, pooled_output, (hidden_states), (attentions)\n",
    "        outputs = self.bert(inputs, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        sequence_output = outputs[0]           \n",
    "        sequence_output = self.dropout(sequence_output, training=training)\n",
    "        logit = self.classifier(sequence_output)\n",
    "        logits = self.crf(logit)\n",
    "        \n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-multilingual-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-multilingual-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "ner_model = TFBertNERClassifier(model_name='bert-base-multilingual-cased',\n",
    "                                  dir_path='bert_ckpt',\n",
    "                                  num_class=len(ner_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_model = ModelWithCRFLoss(ner_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training: Compile tf.keras model with optimizer, loss and learning rate schedule\n",
    "optimizer = tf.keras.optimizers.Adam(3e-5)\n",
    "ner_model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_labels = ['UNK', 'O', 'PDT-B', 'PDT-I', 'MOV-B', 'MOV-I', 'TRV-B', 'TRV-I']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1Metrics(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, x_eval, y_eval):\n",
    "        self.x_eval = x_eval\n",
    "        self.y_eval = y_eval\n",
    "\n",
    "    def compute_f1_pre_rec(self, labels, preds):\n",
    "\n",
    "        return {\n",
    "            \"accuracy\": accuracy_score(labels, preds),\n",
    "            \"precision\": precision_score(labels, preds, suffix=True),\n",
    "            \"recall\": recall_score(labels, preds, suffix=True),\n",
    "            \"f1\": f1_score(labels, preds, suffix=True)\n",
    "        }\n",
    "\n",
    "\n",
    "    def show_report(self, labels, preds):\n",
    "        return classification_report(labels, preds, suffix=True)\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "\n",
    "        results = {}\n",
    "        \n",
    "        pred_argmax = self.model.predict(self.x_eval)\n",
    "        label = self.y_eval\n",
    "\n",
    "        slot_label_map = {i: label for i, label in enumerate(ner_labels)}\n",
    "\n",
    "        out_label_list = [[] for _ in range(label.shape[0])]\n",
    "        preds_list = [[] for _ in range(label.shape[0])]\n",
    "\n",
    "        for i in range(label.shape[0]):\n",
    "            for j in range(label.shape[1]):\n",
    "                if label[i, j] != 0:\n",
    "                    out_label_list[i].append(slot_label_map[label[i][j]])\n",
    "                    if pred_argmax[i][j] ==0:\n",
    "                        pred_argmax[i][j]=1\n",
    "                    preds_list[i].append(slot_label_map[pred_argmax[i][j]])\n",
    "                    \n",
    "        result = self.compute_f1_pre_rec(out_label_list, preds_list)\n",
    "        results.update(result)\n",
    "\n",
    "        print(\"********\")\n",
    "        print(\"F1 Score\")\n",
    "        for key in sorted(results.keys()):\n",
    "            print(\"{}, {:.4f}\".format(key, results[key]))\n",
    "        print(\"\\n\" + self.show_report(out_label_list, preds_list))\n",
    "        print(\"********\")\n",
    "\n",
    "f1_score_callback = F1Metrics(test_inputs, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_out/KOR\\tf2_bert_ner -- Folder already exists \n",
      "\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x000001D1939CE520>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x000001D1939CE520>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:From C:\\Users\\user\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\array_ops.py:5043: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:376: UserWarning: CRF decoding models have serialization issues in TF >=2.5 . Please see isse #2476\n",
      "  return py_builtins.overload_of(f)(*args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "5375/5375 [==============================] - 6673s 1s/step - loss: 16.4787 - accuracy: 0.9893\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "********\n",
      "F1 Score\n",
      "accuracy, 0.7873\n",
      "f1, 0.1058\n",
      "precision, 0.1368\n",
      "recall, 0.0862\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         MOV       0.00      0.00      0.00      1363\n",
      "         PDT       0.16      0.15      0.16      2258\n",
      "         TRV       0.01      0.00      0.00       369\n",
      "\n",
      "   micro avg       0.14      0.09      0.11      3990\n",
      "   macro avg       0.06      0.05      0.05      3990\n",
      "weighted avg       0.09      0.09      0.09      3990\n",
      "\n",
      "********\n",
      "Epoch 2/20\n",
      "5375/5375 [==============================] - 6717s 1s/step - loss: 10.7972 - accuracy: 0.9920\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "********\n",
      "F1 Score\n",
      "accuracy, 0.8390\n",
      "f1, 0.3152\n",
      "precision, 0.3291\n",
      "recall, 0.3025\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         MOV       0.22      0.21      0.21      1363\n",
      "         PDT       0.44      0.40      0.42      2258\n",
      "         TRV       0.08      0.07      0.08       369\n",
      "\n",
      "   micro avg       0.33      0.30      0.32      3990\n",
      "   macro avg       0.25      0.23      0.24      3990\n",
      "weighted avg       0.33      0.30      0.32      3990\n",
      "\n",
      "********\n",
      "Epoch 3/20\n",
      "5375/5375 [==============================] - 6606s 1s/step - loss: 8.1814 - accuracy: 0.9934\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "********\n",
      "F1 Score\n",
      "accuracy, 0.8446\n",
      "f1, 0.3791\n",
      "precision, 0.3989\n",
      "recall, 0.3612\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         MOV       0.25      0.24      0.24      1363\n",
      "         PDT       0.51      0.48      0.49      2258\n",
      "         TRV       0.20      0.09      0.13       369\n",
      "\n",
      "   micro avg       0.40      0.36      0.38      3990\n",
      "   macro avg       0.32      0.27      0.29      3990\n",
      "weighted avg       0.39      0.36      0.37      3990\n",
      "\n",
      "********\n",
      "Epoch 4/20\n",
      "5375/5375 [==============================] - 6602s 1s/step - loss: 6.3011 - accuracy: 0.9946\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "********\n",
      "F1 Score\n",
      "accuracy, 0.8479\n",
      "f1, 0.4352\n",
      "precision, 0.4656\n",
      "recall, 0.4085\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         MOV       0.37      0.32      0.34      1363\n",
      "         PDT       0.52      0.52      0.52      2258\n",
      "         TRV       0.30      0.05      0.09       369\n",
      "\n",
      "   micro avg       0.47      0.41      0.44      3990\n",
      "   macro avg       0.40      0.30      0.32      3990\n",
      "weighted avg       0.45      0.41      0.42      3990\n",
      "\n",
      "********\n",
      "Epoch 5/20\n",
      "5375/5375 [==============================] - 6538s 1s/step - loss: 4.8556 - accuracy: 0.9956\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "********\n",
      "F1 Score\n",
      "accuracy, 0.8351\n",
      "f1, 0.4101\n",
      "precision, 0.3929\n",
      "recall, 0.4288\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         MOV       0.26      0.31      0.29      1363\n",
      "         PDT       0.49      0.55      0.52      2258\n",
      "         TRV       0.22      0.11      0.15       369\n",
      "\n",
      "   micro avg       0.39      0.43      0.41      3990\n",
      "   macro avg       0.32      0.32      0.32      3990\n",
      "weighted avg       0.39      0.43      0.40      3990\n",
      "\n",
      "********\n",
      "Epoch 6/20\n",
      "5375/5375 [==============================] - 6573s 1s/step - loss: 3.7492 - accuracy: 0.9964\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "********\n",
      "F1 Score\n",
      "accuracy, 0.8388\n",
      "f1, 0.4453\n",
      "precision, 0.4405\n",
      "recall, 0.4501\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         MOV       0.35      0.37      0.36      1363\n",
      "         PDT       0.51      0.55      0.53      2258\n",
      "         TRV       0.27      0.12      0.17       369\n",
      "\n",
      "   micro avg       0.44      0.45      0.45      3990\n",
      "   macro avg       0.38      0.35      0.35      3990\n",
      "weighted avg       0.43      0.45      0.44      3990\n",
      "\n",
      "********\n",
      "Epoch 7/20\n",
      "5375/5375 [==============================] - 6580s 1s/step - loss: 2.9966 - accuracy: 0.9971\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "********\n",
      "F1 Score\n",
      "accuracy, 0.8391\n",
      "f1, 0.4506\n",
      "precision, 0.4677\n",
      "recall, 0.4348\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         MOV       0.37      0.33      0.35      1363\n",
      "         PDT       0.54      0.55      0.54      2258\n",
      "         TRV       0.24      0.09      0.14       369\n",
      "\n",
      "   micro avg       0.47      0.43      0.45      3990\n",
      "   macro avg       0.38      0.33      0.34      3990\n",
      "weighted avg       0.45      0.43      0.44      3990\n",
      "\n",
      "********\n",
      "Epoch 8/20\n",
      "5375/5375 [==============================] - 6767s 1s/step - loss: 2.6062 - accuracy: 0.9974\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "********\n",
      "F1 Score\n",
      "accuracy, 0.8261\n",
      "f1, 0.4428\n",
      "precision, 0.4211\n",
      "recall, 0.4669\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         MOV       0.32      0.37      0.34      1363\n",
      "         PDT       0.51      0.57      0.54      2258\n",
      "         TRV       0.22      0.20      0.21       369\n",
      "\n",
      "   micro avg       0.42      0.47      0.44      3990\n",
      "   macro avg       0.35      0.38      0.36      3990\n",
      "weighted avg       0.42      0.47      0.44      3990\n",
      "\n",
      "********\n",
      "Epoch 9/20\n",
      "5375/5375 [==============================] - 6694s 1s/step - loss: 2.1541 - accuracy: 0.9978\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "********\n",
      "F1 Score\n",
      "accuracy, 0.8385\n",
      "f1, 0.4403\n",
      "precision, 0.4844\n",
      "recall, 0.4035\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         MOV       0.36      0.30      0.33      1363\n",
      "         PDT       0.58      0.51      0.54      2258\n",
      "         TRV       0.24      0.11      0.15       369\n",
      "\n",
      "   micro avg       0.48      0.40      0.44      3990\n",
      "   macro avg       0.39      0.31      0.34      3990\n",
      "weighted avg       0.47      0.40      0.43      3990\n",
      "\n",
      "********\n",
      "Epoch 10/20\n",
      "5375/5375 [==============================] - 6552s 1s/step - loss: 1.6404 - accuracy: 0.9983\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "********\n",
      "F1 Score\n",
      "accuracy, 0.8410\n",
      "f1, 0.4454\n",
      "precision, 0.5252\n",
      "recall, 0.3867\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         MOV       0.44      0.24      0.31      1363\n",
      "         PDT       0.57      0.52      0.54      2258\n",
      "         TRV       0.33      0.15      0.20       369\n",
      "\n",
      "   micro avg       0.53      0.39      0.45      3990\n",
      "   macro avg       0.45      0.30      0.35      3990\n",
      "weighted avg       0.50      0.39      0.43      3990\n",
      "\n",
      "********\n",
      "Epoch 11/20\n",
      "5375/5375 [==============================] - 6713s 1s/step - loss: 1.4987 - accuracy: 0.9984\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "********\n",
      "F1 Score\n",
      "accuracy, 0.8408\n",
      "f1, 0.4576\n",
      "precision, 0.4617\n",
      "recall, 0.4536\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         MOV       0.36      0.36      0.36      1363\n",
      "         PDT       0.54      0.55      0.55      2258\n",
      "         TRV       0.28      0.19      0.22       369\n",
      "\n",
      "   micro avg       0.46      0.45      0.46      3990\n",
      "   macro avg       0.39      0.37      0.38      3990\n",
      "weighted avg       0.46      0.45      0.45      3990\n",
      "\n",
      "********\n",
      "Epoch 12/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5375/5375 [==============================] - 6676s 1s/step - loss: 1.4838 - accuracy: 0.9984\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "********\n",
      "F1 Score\n",
      "accuracy, 0.8356\n",
      "f1, 0.4562\n",
      "precision, 0.4807\n",
      "recall, 0.4341\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         MOV       0.40      0.30      0.35      1363\n",
      "         PDT       0.55      0.55      0.55      2258\n",
      "         TRV       0.24      0.21      0.23       369\n",
      "\n",
      "   micro avg       0.48      0.43      0.46      3990\n",
      "   macro avg       0.40      0.35      0.37      3990\n",
      "weighted avg       0.47      0.43      0.45      3990\n",
      "\n",
      "********\n",
      "Epoch 13/20\n",
      "5375/5375 [==============================] - 6747s 1s/step - loss: 1.2198 - accuracy: 0.9986\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "********\n",
      "F1 Score\n",
      "accuracy, 0.8394\n",
      "f1, 0.4554\n",
      "precision, 0.4747\n",
      "recall, 0.4376\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         MOV       0.41      0.30      0.35      1363\n",
      "         PDT       0.53      0.56      0.54      2258\n",
      "         TRV       0.27      0.20      0.23       369\n",
      "\n",
      "   micro avg       0.47      0.44      0.46      3990\n",
      "   macro avg       0.40      0.35      0.37      3990\n",
      "weighted avg       0.46      0.44      0.45      3990\n",
      "\n",
      "********\n",
      "Epoch 14/20\n",
      "5375/5375 [==============================] - 6564s 1s/step - loss: 1.1826 - accuracy: 0.9987\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "********\n",
      "F1 Score\n",
      "accuracy, 0.8423\n",
      "f1, 0.4603\n",
      "precision, 0.4804\n",
      "recall, 0.4419\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         MOV       0.39      0.35      0.37      1363\n",
      "         PDT       0.57      0.53      0.55      2258\n",
      "         TRV       0.23      0.23      0.23       369\n",
      "\n",
      "   micro avg       0.48      0.44      0.46      3990\n",
      "   macro avg       0.40      0.37      0.38      3990\n",
      "weighted avg       0.48      0.44      0.46      3990\n",
      "\n",
      "********\n",
      "Epoch 15/20\n",
      "5375/5375 [==============================] - 6656s 1s/step - loss: 1.2087 - accuracy: 0.9985\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "********\n",
      "F1 Score\n",
      "accuracy, 0.8327\n",
      "f1, 0.4437\n",
      "precision, 0.4507\n",
      "recall, 0.4368\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         MOV       0.34      0.35      0.35      1363\n",
      "         PDT       0.56      0.52      0.54      2258\n",
      "         TRV       0.23      0.24      0.23       369\n",
      "\n",
      "   micro avg       0.45      0.44      0.44      3990\n",
      "   macro avg       0.38      0.37      0.37      3990\n",
      "weighted avg       0.46      0.44      0.45      3990\n",
      "\n",
      "********\n",
      "Epoch 16/20\n",
      "5375/5375 [==============================] - 6558s 1s/step - loss: 0.9184 - accuracy: 0.9989\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "********\n",
      "F1 Score\n",
      "accuracy, 0.8430\n",
      "f1, 0.4516\n",
      "precision, 0.5373\n",
      "recall, 0.3895\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         MOV       0.43      0.33      0.37      1363\n",
      "         PDT       0.62      0.47      0.54      2258\n",
      "         TRV       0.32      0.13      0.18       369\n",
      "\n",
      "   micro avg       0.54      0.39      0.45      3990\n",
      "   macro avg       0.46      0.31      0.36      3990\n",
      "weighted avg       0.53      0.39      0.45      3990\n",
      "\n",
      "********\n",
      "Epoch 17/20\n",
      "5375/5375 [==============================] - 6628s 1s/step - loss: 0.8639 - accuracy: 0.9990\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "********\n",
      "F1 Score\n",
      "accuracy, 0.8358\n",
      "f1, 0.4507\n",
      "precision, 0.4451\n",
      "recall, 0.4564\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         MOV       0.33      0.39      0.36      1363\n",
      "         PDT       0.57      0.53      0.55      2258\n",
      "         TRV       0.22      0.24      0.23       369\n",
      "\n",
      "   micro avg       0.45      0.46      0.45      3990\n",
      "   macro avg       0.37      0.39      0.38      3990\n",
      "weighted avg       0.46      0.46      0.46      3990\n",
      "\n",
      "********\n",
      "Epoch 18/20\n",
      "5375/5375 [==============================] - 6569s 1s/step - loss: 0.8267 - accuracy: 0.9990\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "********\n",
      "F1 Score\n",
      "accuracy, 0.8421\n",
      "f1, 0.4737\n",
      "precision, 0.4986\n",
      "recall, 0.4511\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         MOV       0.37      0.36      0.36      1363\n",
      "         PDT       0.62      0.55      0.58      2258\n",
      "         TRV       0.28      0.21      0.24       369\n",
      "\n",
      "   micro avg       0.50      0.45      0.47      3990\n",
      "   macro avg       0.42      0.37      0.39      3990\n",
      "weighted avg       0.50      0.45      0.47      3990\n",
      "\n",
      "********\n",
      "Epoch 19/20\n",
      "5375/5375 [==============================] - 6683s 1s/step - loss: 0.7371 - accuracy: 0.9991\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "********\n",
      "F1 Score\n",
      "accuracy, 0.8372\n",
      "f1, 0.4548\n",
      "precision, 0.4673\n",
      "recall, 0.4429\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         MOV       0.36      0.38      0.37      1363\n",
      "         PDT       0.56      0.53      0.54      2258\n",
      "         TRV       0.29      0.19      0.23       369\n",
      "\n",
      "   micro avg       0.47      0.44      0.45      3990\n",
      "   macro avg       0.40      0.36      0.38      3990\n",
      "weighted avg       0.47      0.44      0.45      3990\n",
      "\n",
      "********\n",
      "Epoch 20/20\n",
      "5375/5375 [==============================] - 6592s 1s/step - loss: 0.7037 - accuracy: 0.9991\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "********\n",
      "F1 Score\n",
      "accuracy, 0.8405\n",
      "f1, 0.4604\n",
      "precision, 0.4789\n",
      "recall, 0.4434\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         MOV       0.39      0.33      0.36      1363\n",
      "         PDT       0.57      0.55      0.56      2258\n",
      "         TRV       0.23      0.23      0.23       369\n",
      "\n",
      "   micro avg       0.48      0.44      0.46      3990\n",
      "   macro avg       0.40      0.37      0.38      3990\n",
      "weighted avg       0.48      0.44      0.46      3990\n",
      "\n",
      "********\n",
      "{'loss': [16.47867202758789, 10.797248840332031, 8.181421279907227, 6.3011155128479, 4.855599880218506, 3.7492239475250244, 2.996623992919922, 2.606161594390869, 2.1540937423706055, 1.640401840209961, 1.4987406730651855, 1.48381769657135, 1.219828724861145, 1.1825926303863525, 1.208668828010559, 0.918436586856842, 0.8639272451400757, 0.8267378211021423, 0.7370590567588806, 0.7037208676338196], 'accuracy': [0.9893342852592468, 0.9919692277908325, 0.9933692812919617, 0.9946015477180481, 0.995572030544281, 0.9964355230331421, 0.9970709681510925, 0.9974117279052734, 0.9978240132331848, 0.998269259929657, 0.9983854293823242, 0.998380184173584, 0.9986287951469421, 0.9986807703971863, 0.9985405206680298, 0.998948335647583, 0.9989747405052185, 0.9989895820617676, 0.9991030097007751, 0.9990763068199158]}\n"
     ]
    }
   ],
   "source": [
    "model_name = \"tf2_bert_ner\"\n",
    "\n",
    "DATA_IN_PATH = 'data_in/KOR'\n",
    "DATA_OUT_PATH = \"data_out/KOR\"\n",
    "\n",
    "checkpoint_path = os.path.join(DATA_OUT_PATH , model_name, 'weights.h5')\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create path if exists\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    print(\"{} -- Folder already exists \\n\".format(checkpoint_dir))\n",
    "else:\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    print(\"{} -- Folder create complete \\n\".format(checkpoint_dir))\n",
    "    \n",
    "cp_callback = ModelCheckpoint(\n",
    "    checkpoint_path, verbose=1, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "history = ner_model.fit(train_inputs,train_labels, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS,\n",
    "                       callbacks=[cp_callback, f1_score_callback])\n",
    "\n",
    "print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_model.load_weights('save_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_predict(text):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    token_type_ids = []\n",
    "    label_list = []\n",
    "    text_list = []\n",
    "\n",
    "    input_id, attention_mask, token_type_id = bert_tokenizer(text, MAX_LEN)\n",
    "    input_ids.append(input_id)\n",
    "    attention_masks.append(attention_mask)\n",
    "    token_type_ids.append(token_type_id)\n",
    "    \n",
    "    new_input_ids = np.array(input_ids, dtype=int)\n",
    "    new_attention_masks = np.array(attention_masks, dtype=int)\n",
    "    new_type_ids = np.array(token_type_ids, dtype=int)\n",
    "    new_inputs = (new_input_ids, new_attention_masks, new_type_ids)\n",
    "    \n",
    "    predict = ner_model.predict(new_inputs, batch_size=512)\n",
    "    predict_list = predict[0].tolist()\n",
    "    \n",
    "    index = []\n",
    "    for i in predict_list:\n",
    "        index.append(i.index(max(i)))\n",
    "    for i in index:\n",
    "        label_list.append(ner_labels[i])\n",
    "        \n",
    "    for i,j in zip(input_id,label_list):\n",
    "        if j in ['PDT', 'MOV', 'TRV']:\n",
    "            text_list.append([tokenizer.decode(i),j])   \n",
    "    for i,j in text_list:\n",
    "        if i not in [\"[ P A D ]\",\"[ C L S ]\",\"[ S E P ]\",\"[ U N K ]\"]:\n",
    "            print(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_predict(\"깔끔하게 부직포 포장으로 되어 있어서 그냥 뜨거운 물에 풍덩 넣어놓고 좀 휘젓어주면 금방 우러난다. 목욕할 때마다 넣어봤는데(샤워는 자주 해도 목욕은 그렇게 자주가 아님.. 이것도 약재는 약재이므로 용법은 알아서;;)신선한 한약풀 냄새가 욕실에 퍼져서 기분이 좋아졌다. 아직 때가 안 되서 효과까지는 모르겠는데 가려운 피부에도 효과가 있었으면 좋겠네. 박하 같은 것도 팔던데 지금으로서는 대만족이라 다음에는 상쾌하게 박하 사고 싶다. 혹시 오래된 거 팔지 않나 고민했었는데 쑥향기 자체가 페퍼민트처럼 신선하고 포장도 깔끔하고 사용도 간편하고 참.. 우리나라 인터넷 시장도 좋은 거 같다. 주문하니 이렇게 물에 넣기만 하면 되게 딱딱 만들어서 집까지 슝 배달해주고..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
